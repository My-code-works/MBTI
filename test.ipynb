{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Input\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "from keras_contrib.layers.crf import CRF\n",
    "from keras_contrib.utils import save_load_utils\n",
    "from keras_contrib.metrics import crf_accuracy\n",
    "from keras_contrib.losses import crf_loss\n",
    "from keras.utils import to_categorical\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import json\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_json(fn):\n",
    "    with open(fn, 'r', encoding='utf-8') as f:\n",
    "        text = ''.join(f.readlines())\n",
    "    return json.loads(text)\n",
    "\n",
    "\n",
    "class Dataprocessor:\n",
    "    def __init__(self):\n",
    "        self.id2tag = {0: 'PAD', 1: 'B0', 2: 'B1', 3: 'B2', 4: 'B3', 5: 'B4', 6: 'B-1', \n",
    "                       7: 'B-2', 8: 'B-3', 9: 'B-4', 10: 'M', 11: 'E'}\n",
    "        self.tag2id = {}\n",
    "        for i in self.id2tag:\n",
    "            self.tag2id[self.id2tag[i]] = i\n",
    "        self.max_paragraph = 0\n",
    "    \n",
    "    def getid(self, tag):\n",
    "        return self.tag2id[tag]\n",
    "\n",
    "    def gettag(self, id):\n",
    "        return self.id2tag[id]\n",
    "\n",
    "    def section_split(self, datalist):\n",
    "        for js in datalist:\n",
    "            del js[\"simple\"]\n",
    "            new_full = []\n",
    "            for i in js[\"full\"][\"sectionContents\"]:\n",
    "                if len(i[\"text\"]) == 0 or i[\"isContentSection\"] == False:\n",
    "                    continue\n",
    "                i[\"text\"] = i[\"text\"].split('\\n')\n",
    "                new_full.append(i)\n",
    "            self.max_paragraph = max(self.max_paragraph, len(new_full))\n",
    "            js[\"full\"][\"sectionContents\"] = new_full\n",
    "\n",
    "    def tagging(self, data):\n",
    "        for js in data:\n",
    "            last = None\n",
    "            for i in js[\"full\"][\"sectionContents\"]:\n",
    "                i[\"tags\"] = []\n",
    "                i[\"raw_tags\"] = []\n",
    "                for id, x in enumerate(i[\"text\"]):\n",
    "                    if last is None:\n",
    "                        last = i[\"depth\"]\n",
    "                    th = i[\"depth\"] - last\n",
    "                    if id == len(i[\"text\"]) - 1:\n",
    "                        tag = \"E\"\n",
    "                    else:\n",
    "                        tag = \"M\"\n",
    "                    if id == 0:\n",
    "                        tag = \"B%d\"%(th)\n",
    "                    i[\"tags\"].append(self.getid(tag))\n",
    "                    i[\"raw_tags\"].append(tag)\n",
    "                last = i[\"depth\"]\n",
    "    \n",
    "    def load_data(self, jsonfilelist):\n",
    "        article_texts = []\n",
    "        article_tags = []\n",
    "        article_rawtags = []\n",
    "        for f in jsonfilelist:\n",
    "            print('loading %s...' % (os.path.basename(f)))\n",
    "            data = get_json(f)\n",
    "            self.section_split(data)\n",
    "            self.tagging(data)\n",
    "            for article in data:\n",
    "                paragraphs = []\n",
    "                tags = []\n",
    "                rawtags = []\n",
    "                for sec in article['full']['sectionContents']:\n",
    "                    paragraphs.extend(sec['text'])\n",
    "                    tags.extend(sec['tags'])\n",
    "                    rawtags.extend(sec['raw_tags'])\n",
    "                article_texts.append(paragraphs)\n",
    "                article_tags.append(tags)\n",
    "                article_rawtags.append(rawtags)\n",
    "        \n",
    "        print('Finish loading data! Total %d articles.' % (len(article_texts)))\n",
    "        return article_texts, article_tags, article_rawtags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading 0.json...\n",
      "Finish loading data! Total 226 articles.\n",
      "226 226\n",
      "['The 1996 African Cup of Nations was the 20th edition of the Africa Cup of Nations, the soccer championship of Africa (CAF). It was hosted by South Africa, who replaced original hosts Kenya. The field expanded for the first time to sixteen teams, split into four groups of four; the top two teams in each group advancing to the quarterfinals. However, Nigeria withdrew from the tournament at the final moment under pressure from then-dictator Sani Abacha, reducing the field to fifteen. South Africa won its first championship, beating Tunisia in the final 2−0.', '\\xa0Algeria', '\\xa0Angola', '\\xa0Burkina Faso', '\\xa0Cameroon', '\\xa0Ivory Coast', '\\xa0Egypt', '\\xa0Gabon', '\\xa0Ghana', '\\xa0Liberia', '\\xa0Mozambique', '\\xa0Nigeria (holders)*', '\\xa0Sierra Leone', '\\xa0South Africa (hosts)', '\\xa0Tunisia', '\\xa0Zaire', '\\xa0Zambia', \"* Nigeria withdrew prior to the start of the finals. Guinea, as the best side to not qualify, was offered Nigeria's spot in the finals, but declined due to a lack of preparation time.\", 'Teams highlighted in green progress to the Quarter Finals.', 'FNB Stadium, Johannesburg', 'Attendance: 75,000', 'Referee: Said Belqola (Morocco)', 'FNB Stadium, Johannesburg', 'Attendance: 6,000', 'Referee: Sidi Bekaye Magassa (Mali)', 'FNB Stadium, Johannesburg', 'Attendance: 4,000', 'Referee: Lim Kee Chong (Mauritius)', 'FNB Stadium, Johannesburg', 'Attendance: 28,000', 'Referee: Fethi Boucetta (Tunisia)', 'FNB Stadium, Johannesburg', 'Attendance: 20,000', 'Referee: Lucien Bouchardeau (Niger)', 'Kings Park Stadium, Durban', 'Attendance: 6,000', 'Referee: Mohamed Kouradji (Algeria)', 'Free State Stadium, Bloemfontein', 'Attendance: 9,000', 'Referee: Ali Bujsaim (UAE)', 'Free State Stadium, Bloemfontein', 'Attendance: 1,500', 'Referee: Okoampa (Ghana)', 'Free State Stadium, Bloemfontein', 'Attendance: 1,500', 'Referee: Ian McLeod (South Africa)', 'Free State Stadium, Bloemfontein', 'Attendance: 2,000', 'Referee: Masayoshi Okada (Japan)', 'EPRU Stadium, Port Elizabeth', 'Attendance: 180', 'Referee: Charles Masembe (Uganda)', 'Free State Stadium, Bloemfontein', 'Attendance: 200', 'Referee: Lim Kee Chong (Mauritius)', 'Kings Park Stadium, Durban', 'Attendance: 5,000', 'Referee: Gamal Al-Ghandour (Egypt)', 'Kings Park Stadium, Durban', 'Attendance: 4,000', 'Referee: Omer Yengo (Congo)', 'FNB Stadium, Johannesburg', 'Attendance: 3,000', 'Referee: Said Belqola (Morocco)', '\\xa0Nigeria withdrew, so their three matches were canceled.', 'vs. \\xa0Zaire, 16 January 1996', 'vs. \\xa0Liberia, 19 January 1996', 'vs. \\xa0Gabon, 25 January 1996', 'EPRU Stadium, Port Elizabeth', 'Attendance: 8,000', 'Referee: Masayoshi Okada (Japan)', 'EPRU Stadium, Port Elizabeth', 'Attendance: 1,000', 'Referee: Charles Masembe (Uganda)', 'EPRU Stadium, Port Elizabeth', 'Attendance: 1,000', 'Referee: Petros Mathabela (South Africa)', 'EPRU Stadium, Port Elizabeth', 'Attendance: 500', 'Referee: Gamal Al-Ghandour (Egypt)', 'EPRU Stadium, Port Elizabeth', 'Attendance: 1,000', 'Referee: Sidi Bekaye Magassa (Mali)', 'Free State Stadium, Bloemfontein', 'Attendance: 3,500', 'Referee: Ali Bujsaim (UAE)', 'FNB Stadium, Johannesburg', 'Attendance: 80,000', 'Referee: Ali Bujsaim (UAE)', 'Free State Stadium, Bloemfontein', 'Attendance: 8,500', 'Referee: Charles Masembe (Uganda)', 'Kings Park Stadium, Durban', 'Attendance: 4,000', 'Referee: Lim Kee Chong (Mauritius)', 'EPRU Stadium, Port Elizabeth', 'Attendance: 8,000', 'Referee: Sidi Bekaye Magassa (Mali)', 'FNB Stadium, Johannesburg', 'Attendance: 75,000', 'Referee: Gamal Al-Ghandour (Egypt)', 'Kings Park Stadium, Durban', 'Attendance: 5,000', 'Referee: Lucien Bouchardeau (Niger)', 'FNB Stadium, Johannesburg', 'Attendance: 80,000', 'Referee: Omer Yengo (Congo)', 'FNB Stadium, Johannesburg', 'Attendance: 80,000', 'Referee: Charles Masembe (Uganda)', '5 goals', ' Kalusha Bwalya', '4 goals', '', '', ' ', ' Shoes Moshoeu', ' ', ' Mark Williams', ' ', ' ', '3 goals', '', '', ' ', ' Abedi Pele', ' Ahmed El-Kass', ' ', ' Imed Ben Younes', ' ', ' Dennis Lota', ' ', '2 goals', '', '', ' ', ' Ali Meçabih', ' Quinzinho', ' François Omam-Biyik', ' ', ' Brice Mackaya', ' Tony Yeboah', ' Zoubeir Baya', ' ', ' Adel Sellimi', ' Johnson Bwalya', ' Kenneth Malitoli', ' ', '1 goal', '', '', ' ', ' Billel Dziri', ' Tarek Lazizi', ' Khaled Lounici', ' Joni', ' Paulão', ' Aboubakari Ouédraogo', ' Youssouf Traoré', ' Boureima Zongo', ' Georges Mouyémé', ' Alphonse Tchami', ' Joël Tiéhi', ' Moussa Traoré', ' ', ' Samir Ibrahim', ' Ali Maher', ' Aurelien Bekogo', ' Guy Nzeng', ' Felix Aboagye', ' Charles Akonnor', ' Kwame Ayew', ' Mass Sarr Jr.', ' Kelvin Sebwe', ' Tico-Tico', ' Mohamed Kallon', ' John Gbassay Sessay', ' ', ' Shaun Bartlett', ' Mark Fish', ' Phil Masinga', ' Abdelkader Ben Hassen', ' Hédi Berkhissa', ' Kaies Ghodhbane', ' Liombi Essende', ' Roger Lukaku', ' Elijah Litana', ' Hillary Makasa', ' Vincent Mutale', ' ', 'Own goal', ' Helder Vicente', 'Goalkeeper', ' Chokri El Ouaer', 'Defenders', ' Yasser Radwan', ' Mark Fish', ' Elijah Litana', ' Isaac Asare', 'Midfielders', ' Zoubeir Baya', ' Hazem Emam', ' Abedi Pele', ' Mark Williams', 'Forwards', ' Kalusha Bwalya', ' Tony Yeboah'] [1, 2, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 1, 2, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 1, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 1, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 1, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 1, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 1, 10, 10, 10, 10, 11, 1, 10, 11, 1, 10, 11, 6, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 1, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11]\n"
     ]
    }
   ],
   "source": [
    "data_path = './data/'\n",
    "filelist = [(data_path + '%d.json' % i) for i in range(1)]\n",
    "processor = Dataprocessor()\n",
    "train_texts, train_tags, train_rawtags = processor.load_data(filelist)\n",
    "print(len(train_texts), len(train_tags))\n",
    "print(train_texts[1], train_tags[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading 0.json...\n",
      "loading 1.json...\n",
      "loading 2.json...\n",
      "loading 3.json...\n",
      "loading 4.json...\n",
      "loading 5.json...\n",
      "loading 6.json...\n",
      "loading 7.json...\n",
      "loading 8.json...\n",
      "loading 9.json...\n",
      "loading 10.json...\n",
      "loading 11.json...\n",
      "loading 12.json...\n",
      "loading 13.json...\n",
      "loading 14.json...\n",
      "loading 15.json...\n",
      "loading 16.json...\n",
      "loading 17.json...\n",
      "loading 18.json...\n",
      "loading 19.json...\n",
      "loading 20.json...\n",
      "loading 21.json...\n",
      "loading 22.json...\n",
      "loading 23.json...\n",
      "loading 24.json...\n",
      "loading 25.json...\n",
      "loading 26.json...\n",
      "loading 27.json...\n",
      "loading 28.json...\n",
      "loading 29.json...\n",
      "loading 30.json...\n",
      "loading 31.json...\n",
      "loading 32.json...\n",
      "loading 33.json...\n",
      "loading 34.json...\n",
      "loading 35.json...\n",
      "loading 36.json...\n",
      "loading 37.json...\n",
      "loading 38.json...\n",
      "loading 39.json...\n",
      "loading 40.json...\n",
      "loading 41.json...\n",
      "loading 42.json...\n",
      "loading 43.json...\n",
      "loading 44.json...\n",
      "loading 45.json...\n",
      "loading 46.json...\n",
      "loading 47.json...\n",
      "loading 48.json...\n",
      "loading 49.json...\n",
      "loading 50.json...\n",
      "loading 51.json...\n",
      "loading 52.json...\n",
      "loading 53.json...\n",
      "loading 54.json...\n",
      "loading 55.json...\n",
      "loading 56.json...\n",
      "loading 57.json...\n",
      "loading 58.json...\n",
      "loading 59.json...\n",
      "loading 60.json...\n",
      "loading 61.json...\n",
      "loading 62.json...\n",
      "loading 63.json...\n",
      "loading 64.json...\n",
      "loading 65.json...\n",
      "loading 66.json...\n",
      "loading 67.json...\n",
      "loading 68.json...\n",
      "loading 69.json...\n",
      "loading 70.json...\n",
      "loading 71.json...\n",
      "loading 72.json...\n",
      "loading 73.json...\n",
      "loading 74.json...\n",
      "loading 75.json...\n",
      "loading 76.json...\n",
      "loading 77.json...\n",
      "loading 78.json...\n",
      "loading 79.json...\n",
      "loading 80.json...\n",
      "loading 81.json...\n",
      "loading 82.json...\n",
      "loading 83.json...\n",
      "loading 84.json...\n",
      "loading 85.json...\n",
      "loading 86.json...\n",
      "loading 87.json...\n",
      "loading 88.json...\n",
      "loading 89.json...\n",
      "loading 90.json...\n",
      "loading 91.json...\n",
      "loading 92.json...\n",
      "loading 93.json...\n",
      "loading 94.json...\n",
      "loading 95.json...\n",
      "loading 96.json...\n",
      "loading 97.json...\n",
      "loading 98.json...\n",
      "loading 99.json...\n",
      "loading 100.json...\n",
      "loading 101.json...\n",
      "loading 102.json...\n",
      "loading 103.json...\n",
      "loading 104.json...\n",
      "loading 105.json...\n",
      "loading 106.json...\n",
      "loading 107.json...\n",
      "loading 108.json...\n",
      "loading 109.json...\n",
      "loading 110.json...\n",
      "loading 111.json...\n",
      "loading 112.json...\n",
      "loading 113.json...\n",
      "loading 114.json...\n",
      "loading 115.json...\n",
      "loading 116.json...\n",
      "loading 117.json...\n",
      "loading 118.json...\n",
      "loading 119.json...\n",
      "loading 120.json...\n",
      "loading 121.json...\n",
      "loading 122.json...\n",
      "loading 123.json...\n",
      "loading 124.json...\n",
      "loading 125.json...\n",
      "loading 126.json...\n",
      "loading 127.json...\n",
      "loading 128.json...\n",
      "loading 129.json...\n",
      "loading 130.json...\n",
      "loading 131.json...\n",
      "loading 132.json...\n",
      "loading 133.json...\n",
      "loading 134.json...\n",
      "loading 135.json...\n",
      "loading 136.json...\n",
      "loading 137.json...\n",
      "loading 138.json...\n",
      "loading 139.json...\n",
      "loading 140.json...\n",
      "loading 141.json...\n",
      "loading 142.json...\n",
      "loading 143.json...\n",
      "loading 144.json...\n",
      "loading 145.json...\n",
      "loading 146.json...\n",
      "loading 147.json...\n",
      "loading 148.json...\n",
      "loading 149.json...\n",
      "loading 150.json...\n",
      "loading 151.json...\n",
      "loading 152.json...\n",
      "loading 153.json...\n",
      "loading 154.json...\n",
      "loading 155.json...\n",
      "loading 156.json...\n",
      "loading 157.json...\n",
      "loading 158.json...\n",
      "loading 159.json...\n",
      "loading 160.json...\n",
      "loading 161.json...\n",
      "loading 162.json...\n",
      "loading 163.json...\n",
      "loading 164.json...\n",
      "loading 165.json...\n",
      "loading 166.json...\n",
      "loading 167.json...\n",
      "loading 168.json...\n",
      "loading 169.json...\n",
      "loading 170.json...\n",
      "loading 171.json...\n",
      "loading 172.json...\n",
      "loading 173.json...\n",
      "loading 174.json...\n",
      "loading 175.json...\n",
      "loading 176.json...\n",
      "loading 177.json...\n",
      "loading 178.json...\n",
      "loading 179.json...\n",
      "loading 180.json...\n",
      "loading 181.json...\n",
      "loading 182.json...\n",
      "loading 183.json...\n",
      "loading 184.json...\n",
      "loading 185.json...\n",
      "loading 186.json...\n",
      "loading 187.json...\n",
      "loading 188.json...\n",
      "loading 189.json...\n",
      "loading 190.json...\n",
      "loading 191.json...\n",
      "loading 192.json...\n",
      "loading 193.json...\n",
      "loading 194.json...\n",
      "loading 195.json...\n",
      "loading 196.json...\n",
      "loading 197.json...\n",
      "loading 198.json...\n",
      "loading 199.json...\n",
      "loading 200.json...\n",
      "loading 201.json...\n",
      "loading 202.json...\n",
      "loading 203.json...\n",
      "loading 204.json...\n",
      "loading 205.json...\n",
      "loading 206.json...\n",
      "loading 207.json...\n",
      "loading 208.json...\n",
      "loading 209.json...\n",
      "loading 210.json...\n",
      "loading 211.json...\n",
      "loading 212.json...\n",
      "loading 213.json...\n",
      "loading 214.json...\n",
      "loading 215.json...\n",
      "loading 216.json...\n",
      "loading 217.json...\n",
      "loading 218.json...\n",
      "loading 219.json...\n",
      "loading 220.json...\n",
      "loading 221.json...\n",
      "loading 222.json...\n",
      "loading 223.json...\n",
      "loading 224.json...\n",
      "loading 225.json...\n",
      "loading 226.json...\n",
      "loading 227.json...\n",
      "loading 228.json...\n",
      "loading 229.json...\n",
      "loading 230.json...\n",
      "loading 231.json...\n",
      "loading 232.json...\n",
      "loading 233.json...\n",
      "loading 234.json...\n",
      "loading 235.json...\n",
      "loading 236.json...\n",
      "loading 237.json...\n",
      "loading 238.json...\n",
      "loading 239.json...\n",
      "loading 240.json...\n",
      "loading 241.json...\n",
      "loading 242.json...\n",
      "loading 243.json...\n",
      "loading 244.json...\n",
      "loading 245.json...\n",
      "loading 246.json...\n",
      "loading 247.json...\n",
      "loading 248.json...\n",
      "loading 249.json...\n",
      "loading 250.json...\n",
      "loading 251.json...\n",
      "loading 252.json...\n",
      "loading 253.json...\n",
      "loading 254.json...\n",
      "loading 255.json...\n",
      "loading 256.json...\n",
      "loading 257.json...\n",
      "loading 258.json...\n",
      "loading 259.json...\n",
      "loading 260.json...\n",
      "loading 261.json...\n",
      "loading 262.json...\n",
      "loading 263.json...\n",
      "loading 264.json...\n",
      "loading 265.json...\n",
      "loading 266.json...\n",
      "loading 267.json...\n",
      "loading 268.json...\n",
      "loading 269.json...\n",
      "loading 270.json...\n",
      "loading 271.json...\n",
      "loading 272.json...\n",
      "loading 273.json...\n",
      "loading 274.json...\n",
      "loading 275.json...\n",
      "loading 276.json...\n",
      "loading 277.json...\n",
      "loading 278.json...\n",
      "loading 279.json...\n",
      "loading 280.json...\n",
      "loading 281.json...\n",
      "loading 282.json...\n",
      "loading 283.json...\n",
      "loading 284.json...\n",
      "loading 285.json...\n",
      "loading 286.json...\n",
      "loading 287.json...\n",
      "loading 288.json...\n",
      "loading 289.json...\n",
      "loading 290.json...\n",
      "loading 291.json...\n",
      "loading 292.json...\n",
      "loading 293.json...\n",
      "loading 294.json...\n",
      "loading 295.json...\n",
      "loading 296.json...\n",
      "loading 297.json...\n",
      "loading 298.json...\n",
      "loading 299.json...\n",
      "loading 300.json...\n",
      "loading 301.json...\n",
      "loading 302.json...\n",
      "loading 303.json...\n",
      "loading 304.json...\n",
      "loading 305.json...\n",
      "loading 306.json...\n",
      "loading 307.json...\n",
      "loading 308.json...\n",
      "loading 309.json...\n",
      "loading 310.json...\n",
      "loading 311.json...\n",
      "loading 312.json...\n",
      "loading 313.json...\n",
      "loading 314.json...\n",
      "loading 315.json...\n",
      "loading 316.json...\n",
      "loading 317.json...\n",
      "loading 318.json...\n",
      "loading 319.json...\n",
      "loading 320.json...\n",
      "loading 321.json...\n",
      "loading 322.json...\n",
      "loading 323.json...\n",
      "loading 324.json...\n",
      "loading 325.json...\n",
      "loading 326.json...\n",
      "loading 327.json...\n",
      "loading 328.json...\n",
      "loading 329.json...\n",
      "loading 330.json...\n",
      "loading 331.json...\n",
      "loading 332.json...\n",
      "loading 333.json...\n",
      "loading 334.json...\n",
      "loading 335.json...\n",
      "loading 336.json...\n",
      "loading 337.json...\n",
      "loading 338.json...\n",
      "loading 339.json...\n",
      "loading 340.json...\n",
      "loading 341.json...\n",
      "loading 342.json...\n",
      "loading 343.json...\n",
      "loading 344.json...\n",
      "loading 345.json...\n",
      "loading 346.json...\n",
      "loading 347.json...\n",
      "loading 348.json...\n",
      "loading 349.json...\n",
      "loading 350.json...\n",
      "loading 351.json...\n",
      "loading 352.json...\n",
      "loading 353.json...\n",
      "loading 354.json...\n",
      "loading 355.json...\n",
      "loading 356.json...\n",
      "loading 357.json...\n",
      "loading 358.json...\n",
      "loading 359.json...\n",
      "loading 360.json...\n",
      "loading 361.json...\n",
      "loading 362.json...\n",
      "loading 363.json...\n",
      "loading 364.json...\n",
      "loading 365.json...\n",
      "loading 366.json...\n",
      "loading 367.json...\n",
      "loading 368.json...\n",
      "loading 369.json...\n",
      "loading 370.json...\n",
      "loading 371.json...\n",
      "loading 372.json...\n",
      "loading 373.json...\n",
      "loading 374.json...\n",
      "loading 375.json...\n",
      "loading 376.json...\n",
      "loading 377.json...\n",
      "loading 378.json...\n",
      "loading 379.json...\n",
      "loading 380.json...\n",
      "loading 381.json...\n",
      "loading 382.json...\n",
      "loading 383.json...\n",
      "loading 384.json...\n",
      "loading 385.json...\n",
      "loading 386.json...\n",
      "loading 387.json...\n",
      "loading 388.json...\n",
      "loading 389.json...\n",
      "loading 390.json...\n",
      "loading 391.json...\n",
      "loading 392.json...\n",
      "loading 393.json...\n",
      "loading 394.json...\n",
      "loading 395.json...\n",
      "loading 396.json...\n",
      "loading 397.json...\n",
      "loading 398.json...\n",
      "loading 399.json...\n",
      "loading 400.json...\n",
      "loading 401.json...\n",
      "loading 402.json...\n",
      "loading 403.json...\n",
      "loading 404.json...\n",
      "loading 405.json...\n",
      "loading 406.json...\n",
      "loading 407.json...\n",
      "loading 408.json...\n",
      "loading 409.json...\n",
      "loading 410.json...\n",
      "loading 411.json...\n",
      "loading 412.json...\n",
      "loading 413.json...\n",
      "loading 414.json...\n",
      "loading 415.json...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading 416.json...\n",
      "loading 417.json...\n",
      "loading 418.json...\n",
      "loading 419.json...\n",
      "loading 420.json...\n",
      "loading 421.json...\n",
      "loading 422.json...\n",
      "loading 423.json...\n",
      "loading 424.json...\n",
      "loading 425.json...\n",
      "loading 426.json...\n",
      "loading 427.json...\n",
      "loading 428.json...\n",
      "loading 429.json...\n",
      "loading 430.json...\n",
      "loading 431.json...\n",
      "loading 432.json...\n",
      "loading 433.json...\n",
      "loading 434.json...\n",
      "loading 435.json...\n",
      "loading 436.json...\n",
      "loading 437.json...\n",
      "loading 438.json...\n",
      "loading 439.json...\n",
      "loading 440.json...\n",
      "loading 441.json...\n",
      "loading 442.json...\n",
      "loading 443.json...\n",
      "loading 444.json...\n",
      "loading 445.json...\n",
      "loading 446.json...\n",
      "loading 447.json...\n",
      "loading 448.json...\n",
      "loading 449.json...\n",
      "loading 450.json...\n",
      "loading 451.json...\n",
      "loading 452.json...\n",
      "loading 453.json...\n",
      "loading 454.json...\n",
      "loading 455.json...\n",
      "loading 456.json...\n",
      "loading 457.json...\n",
      "loading 458.json...\n",
      "loading 459.json...\n",
      "loading 460.json...\n",
      "loading 461.json...\n",
      "loading 462.json...\n",
      "loading 463.json...\n",
      "loading 464.json...\n",
      "loading 465.json...\n",
      "loading 466.json...\n",
      "loading 467.json...\n",
      "loading 468.json...\n",
      "loading 469.json...\n",
      "loading 470.json...\n",
      "loading 471.json...\n",
      "loading 472.json...\n",
      "loading 473.json...\n",
      "loading 474.json...\n",
      "loading 475.json...\n",
      "loading 476.json...\n",
      "loading 477.json...\n",
      "loading 478.json...\n",
      "loading 479.json...\n",
      "loading 480.json...\n",
      "loading 481.json...\n",
      "loading 482.json...\n",
      "loading 483.json...\n",
      "loading 484.json...\n",
      "loading 485.json...\n",
      "loading 486.json...\n",
      "loading 487.json...\n",
      "loading 488.json...\n",
      "loading 489.json...\n",
      "loading 490.json...\n",
      "loading 491.json...\n",
      "loading 492.json...\n",
      "loading 493.json...\n",
      "loading 494.json...\n",
      "loading 495.json...\n",
      "loading 496.json...\n",
      "loading 497.json...\n",
      "loading 498.json...\n",
      "loading 499.json...\n",
      "Finish loading data! Total 114975 articles.\n"
     ]
    }
   ],
   "source": [
    "from Dataprocessor import Dataprocessor\n",
    "\n",
    "filelist = [('data/%d.json' % i) for i in range(500)]\n",
    "processor = Dataprocessor()\n",
    "train_texts, train_tags, train_rawtags = processor.load_data(filelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 8520 paragraphs\n",
      "WARNING:tensorflow:Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7f68eb5f39d8>) includes params argument, but params are not passed to Estimator.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp476qtmau\n",
      "WARNING:tensorflow:eval_on_tpu ignored because use_tpu is False.\n",
      "0 ...\n",
      "2000 ...\n",
      "4000 ...\n",
      "6000 ...\n",
      "8000 ...\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "from bert_utils import get_all_features\n",
    "import os\n",
    "BERT_BASE = os.path.join(os.getcwd(), 'bert/bert_model/uncased_L-12_H-768_A-12')\n",
    "\n",
    "sample_sum = 200\n",
    "\n",
    "bert_config_file = os.path.join(BERT_BASE, 'bert_config.json')\n",
    "vocab_file = os.path.join(BERT_BASE, 'vocab.txt')\n",
    "bert_checkpoint = os.path.join(BERT_BASE, 'bert_model.ckpt')\n",
    "    \n",
    "feature = get_all_features(train_texts[0:sample_sum], bert_config_file, vocab_file, bert_checkpoint)\n",
    "print(len(feature))\n",
    "\n",
    "# 8520: paragraphs\n",
    "# 6000: \n",
    "# 0: item\n",
    "# 1: 14.. sentence per item\n",
    "# 2: 768 vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14,)\n",
      "(200,)\n",
      "(100,)\n",
      "[\"The 1992 Republican National Convention was held in the Astrodome in Houston, Texas, from August 17 to August 20, 1992. The convention nominated President George H. W. Bush and Vice President Dan Quayle for reelection. It was Bush's fourth consecutive appearance as a candidate on a major party ticket; only Bush and Franklin D. Roosevelt have been nominated on four consecutive presidential tickets. Richard M. Nixon and Roosevelt were nominated five times, but not consecutively.\", \"The convention is notable in that it featured the last major address of former President and Bush's Predecessor Ronald Reagan's long political career. In his speech, Reagan told Americans that:\", \"Whatever else history may say about me when I'm gone, I hope it will record that I appealed to your best hopes, not your worst fears, to your confidence rather than your doubts. My dream is that you will travel the road ahead with liberty's lamp guiding your steps and opportunity's arm steadying your way. My fondest hope for each one of you—and especially for the young people here—is that you will love your country, not for her power or wealth, but for her selflessness and her idealism. May each of you have the heart to conceive, the understanding to direct, and the hand to execute works that will make the world a little better for your having been here.\", 'As the economy was in a recession and domestic affairs in general had dramatically decayed, the GOP lagged in the polls by double digits behind the Bill Clinton–Al Gore Democratic ticket after a successful Democratic Convention, and with Ross Perot temporarily out of the race, the Republican Party worked hard to rally its base of social conservatives. Pat Buchanan\\'s opening night \"Culture War\" speech argued that a great battle of values was taking place in the United States. Republican National Committee chairman Rich Bond (referring to Democrats) claimed that \"we are America, they are not America.\" Marilyn Quayle dismissed Bill Clinton\\'s claim to a new generation of leadership by saying, \"Not everyone demonstrated, dropped out, took drugs, joined in the sexual revolution or dodged the draft.\" Regarding Buchanan\\'s speech, liberal humorist Molly Ivins quipped that it \"probably sounded better in the original German.\"  Writing twenty years after the convention, the New York Times wrote, \"Supporters of Mr. Bush pointed to the tone of the convention as one of the reasons he lost re-election that November to Bill Clinton.\", as it centered more on Reagan-era values and Bush\\'s international credentials at a time that the main issue was the domestic crisis. The fact that the now-infamous \"No new taxes\" pledge had haunted the President for the last three years, the economy was barely mentioned.', \"AIDS activist Mary Fisher, who has HIV, addressed the convention, making an eloquent plea for her cause. (She also addressed the 1996 RNC). Her 1992 speech was listed as #50 in American Rhetoric's Top 100 Speeches of the 20th Century (listed by rank).\", \"During his acceptance speech, President Bush thanked former President Richard Nixon for his advice and contributions to the administration's foreign policy. This would be Nixon's last RNC, as he died in 1994.\", 'Restauranteur Ninfa Laurenzo delivered the Pledge of Allegiance at the opening session on August 17, 1992.', \"The stadium banned outside food from the convention, but set up a food court in the nearby Astroarena. The food court operations included Atchafalaya River Cafe, Bambolino's, Frenchy's, Luther's Bar-B-Q, Ninfa's, PeaColes, and Tommy's Burgers. Some restaurant owners had connections with the Houston Host Committee, the group in charge of the vendor market of the Astroarena, and Republican Party officials. The hot dogs and soft drinks sold by the Astrodome's official caterer, Harry M. Stevens, were not present during the convention. Vendors at the convention paid Stevens a fee so they could sell food at the convention, as specified in Stevens' contract with the Astrodome.\", 'The convention energized the Republican base, giving the Bush-Quayle ticket a bounce in the polls. As the bounce faded, the race returned to a lopsided double-digit Clinton-Gore lead. The race narrowed considerably, however, when Ross Perot rebooted his insurgent campaign.', \"Because the Astrodome was their home stadium, the Houston Astros were forced to play 26 consecutive road games from July 27 through August 23. The National Football League's Houston Oilers would also be forced to play all their preseason games on the road. The major parties have avoided hosting their conventions at baseball stadiums since then, now holding them in non sports venues (such as Convention Centers) and venues for teams whose seasons are not currently in play at the time of the convention.\", 'President Bush 21', 'Patrick J. Buchanan 18', 'former ambassador Alan Keyes 1', 'Dan Quayle was renominated by voice vote.']\n",
      "[1, 2, 10, 10, 10, 10, 10, 11, 1, 11, 2, 10, 11, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(np.array(train_texts[0]).shape)\n",
    "print(np.array(feature).shape)\n",
    "print(np.array(train_tags[0]).shape)\n",
    "print(train_texts[0])\n",
    "print(train_tags[0])\n",
    "\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMmodel:\n",
    "    def __init__(self, input_length, para_emb_dim, num_tags, hidden_dim=200, dropout=0.5):\n",
    "        self.num_tags = num_tags\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Bidirectional(LSTM(hidden_dim, return_sequences=True), input_shape=(input_length, para_emb_dim)))\n",
    "        self.model.add(Dropout(dropout))\n",
    "        # self.model.add(Bidirectional(LSTM(hidden_dim, return_sequences=True), input_shape=(input_length, para_emb_dim)))\n",
    "        # self.model.add(Dropout(dropout))\n",
    "        self.model.add(TimeDistributed(Dense(self.num_tags)))\n",
    "        crf = CRF(self.num_tags)\n",
    "        self.model.add(crf)\n",
    "        self.model.compile('rmsprop', loss=crf_loss, metrics=[crf_accuracy])\n",
    "    \n",
    "    def save_model(self, filepath):\n",
    "        save_load_utils.save_all_weights(self.model, filepath)\n",
    "    \n",
    "    def restore_model(self, filepath):\n",
    "        save_load_utils.load_all_weights(self.model, filepath)\n",
    "        \n",
    "    def train(self, trainX, trainY, batch_size=32, epochs=10, validation_split=0.1, verbose=1):\n",
    "        return self.model.fit(trainX, np.array(trainY), batch_size=batch_size, epochs=epochs, \n",
    "                             validation_split=validation_split, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_1 (Bidirection (None, 100, 400)          1550400   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100, 400)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 100, 12)           4812      \n",
      "_________________________________________________________________\n",
      "crf_1 (CRF)                  (None, 100, 12)           324       \n",
      "=================================================================\n",
      "Total params: 1,555,536\n",
      "Trainable params: 1,555,536\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "INPUT_LENGTH = 100\n",
    "PARAGRAPH_EMB_DIM = 768\n",
    "NUM_TAGS = 12\n",
    "\n",
    "model = LSTMmodel(INPUT_LENGTH, PARAGRAPH_EMB_DIM, NUM_TAGS)\n",
    "model.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "import numpy as np\n",
    "\n",
    "tags = train_tags[0:sample_sum]\n",
    "X, rawY = [], [] # X is 3D: article, paragraph, embedding; Y is 2D: article, paragraph\n",
    "for f, t in zip(feature, tags):\n",
    "    \n",
    "    while len(f) < INPUT_LENGTH:\n",
    "        f.append(np.zeros(PARAGRAPH_EMB_DIM))\n",
    "        t.append(0)\n",
    "    f = f[0:INPUT_LENGTH]\n",
    "    t = t[0:INPUT_LENGTH]\n",
    "    X.append(f)\n",
    "    rawY.append(t)\n",
    "    \n",
    "Y = [to_categorical(y, num_classes=NUM_TAGS) for y in rawY] # Y is now 3D\n",
    "\n",
    "data_size = len(X)\n",
    "train_size = int(data_size * 0.9)\n",
    "trainX, trainY = X[:train_size], Y[:train_size]\n",
    "testX, testY = X[train_size:], Y[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 162 samples, validate on 18 samples\n",
      "Epoch 1/10\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 1.6966 - crf_accuracy: 0.5495 - val_loss: 0.5677 - val_crf_accuracy: 0.8722\n",
      "Epoch 2/10\n",
      "162/162 [==============================] - 3s 18ms/step - loss: 0.4904 - crf_accuracy: 0.8499 - val_loss: 0.4568 - val_crf_accuracy: 0.8344\n",
      "Epoch 3/10\n",
      "162/162 [==============================] - 3s 18ms/step - loss: 0.4217 - crf_accuracy: 0.8590 - val_loss: 0.5198 - val_crf_accuracy: 0.8767\n",
      "Epoch 4/10\n",
      "162/162 [==============================] - 3s 17ms/step - loss: 0.4179 - crf_accuracy: 0.8601 - val_loss: 0.4349 - val_crf_accuracy: 0.8783\n",
      "Epoch 5/10\n",
      "162/162 [==============================] - 3s 17ms/step - loss: 0.4039 - crf_accuracy: 0.8638 - val_loss: 0.3856 - val_crf_accuracy: 0.8800\n",
      "Epoch 6/10\n",
      "162/162 [==============================] - 3s 17ms/step - loss: 0.3831 - crf_accuracy: 0.8640 - val_loss: 0.3850 - val_crf_accuracy: 0.8833\n",
      "Epoch 7/10\n",
      "162/162 [==============================] - 3s 17ms/step - loss: 0.3781 - crf_accuracy: 0.8636 - val_loss: 0.3770 - val_crf_accuracy: 0.8850\n",
      "Epoch 8/10\n",
      "162/162 [==============================] - 3s 18ms/step - loss: 0.3686 - crf_accuracy: 0.8645 - val_loss: 0.4522 - val_crf_accuracy: 0.8583\n",
      "Epoch 9/10\n",
      "162/162 [==============================] - 3s 17ms/step - loss: 0.3784 - crf_accuracy: 0.8659 - val_loss: 0.3779 - val_crf_accuracy: 0.8656\n",
      "Epoch 10/10\n",
      "162/162 [==============================] - 3s 18ms/step - loss: 0.3539 - crf_accuracy: 0.8684 - val_loss: 0.3833 - val_crf_accuracy: 0.8528\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "history = model.model.fit(np.array(trainX), np.array(trainY), batch_size=32, epochs=10, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "20/20 [==============================] - 1s 33ms/step\n"
     ]
    }
   ],
   "source": [
    "# Predict on test\n",
    "test_pred = model.model.predict(np.array(testX), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "409 326 0.5564625850340136\n"
     ]
    }
   ],
   "source": [
    "truecnt = 0\n",
    "falsecnt = 0\n",
    "for (i, pred) in enumerate(test_pred):\n",
    "    for j, p in enumerate(pred):\n",
    "        if np.argmax(testY[i][j]) != 0:\n",
    "            if np.argmax(p) == np.argmax(testY[i][j]):\n",
    "                truecnt += 1\n",
    "            else:\n",
    "                falsecnt += 1\n",
    "print(truecnt, falsecnt, truecnt/(truecnt+falsecnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model(\"model_save/init.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "# plt.style.use(\"ggplot\")\n",
    "# plt.figure(figsize=(12,12))\n",
    "# plt.plot(history[\"acc\"])\n",
    "# plt.plot(history[\"val_acc\"])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def save_train_tsv(texts, tags):\n",
    "    \n",
    "    file_path = \"./data/train.tsv\"\n",
    "\n",
    "    with open(file_path, 'w') as file:\n",
    "        tsv_writer = csv.writer(file, delimiter='\\t')\n",
    "        \n",
    "        for text, tag in zip(texts, tags):\n",
    "            for i in range(len(text)):\n",
    "                tsv_writer.writerow([text[i], tag[i]])\n",
    "\n",
    "save_train_tsv(train_texts, train_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"The 1992 Republican National Convention was held in the Astrodome in Houston, Texas, from August 17 to August 20, 1992. The convention nominated President George H. W. Bush and Vice President Dan Quayle for reelection. It was Bush's fourth consecutive appearance as a candidate on a major party ticket; only Bush and Franklin D. Roosevelt have been nominated on four consecutive presidential tickets. Richard M. Nixon and Roosevelt were nominated five times, but not consecutively.\", '1']\n"
     ]
    }
   ],
   "source": [
    "def read_tsv():\n",
    "    \n",
    "    file_path = \"./data/train.tsv\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        reader = csv.reader(file, delimiter='\\t')\n",
    "        for row in reader:\n",
    "            print(row)\n",
    "            break\n",
    "\n",
    "read_tsv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'modeling'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-da4902fb56bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodeling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_classifier\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataProcessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdata_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./data/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jiahaolin19971119/Wiki-segmentor/bert/run_classifier.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmodeling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtokenization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'modeling'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from bert import modeling\n",
    "\n",
    "from bert.run_classifier import DataProcessor\n",
    "\n",
    "data_dir = './data/'\n",
    "\n",
    "class MyProcessor(DataProcessor):\n",
    "    \n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"dev.tsv\")), \"dev\")\n",
    "\n",
    "    def get_test_examples(self, data_dir):\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"test.tsv\")), \"test\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        return [\"0\", \"1\", \"2\", \"3\"]\n",
    "\n",
    "    def _create_examples(self, lines, set_type):\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            guid = \"%s-%s\" % (set_type, i)\n",
    "            if set_type == \"test\":\n",
    "                text_a = tokenization.convert_to_unicode(line[0])\n",
    "                label = \"0\"\n",
    "            else:\n",
    "                text_a = tokenization.convert_to_unicode(line[1])\n",
    "                label = tokenization.convert_to_unicode(line[0])\n",
    "            examples.append(InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n",
    "        return examples\n",
    "\n",
    "class EditTsv:\n",
    "    \n",
    "    file_path = \"./data/\"\n",
    "    train_rate = 0.8\n",
    "    test_rate = 0.2\n",
    "\n",
    "    @staticmethod\n",
    "    def read_tsv():\n",
    "\n",
    "        with open(EditTsv.file_path + 'train.tsv', 'r') as file:\n",
    "            reader = csv.reader(tsvfile, delimiter='\\t')\n",
    "            for row in reader:\n",
    "                print(row)\n",
    "                input()\n",
    "\n",
    "    @staticmethod\n",
    "    def save_train_tsv(texts, tags):\n",
    "        \n",
    "        with open(EditTsv.file_path + 'train.tsv', 'w') as file:\n",
    "            tsv_writer = csv.writer(file, delimiter='\\t')\n",
    "            for text, tag in zip(texts, tags):\n",
    "                for i in len(text):\n",
    "                    tsv_writer.writerow([text[i], tag[i]])\n",
    "\n",
    "    @staticmethod\n",
    "    def save_dev_tsv(texts, tags):\n",
    "\n",
    "        with open(file_path + 'dev.tsv', 'w') as file:\n",
    "            tsv_writer = csv.writer(file, delimiter='\\t')\n",
    "            for text, tag in zip(texts, tags):\n",
    "                for i in len(text):\n",
    "                    tsv_writer.writerow([text[i], tag[i]])\n",
    "    \n",
    "    @staticmethod\n",
    "    def save_test_tsv(texts, tags):    \n",
    "        \n",
    "        with open(EditTsv.file_path + 'test.tsv', 'w') as file:\n",
    "            tsv_writer = csv.writer(file, delimiter='\\t')\n",
    "            for text, tag in zip(texts, tags):\n",
    "                for i in len(text):\n",
    "                    tsv_writer.writerow([text[i], tag[i]])\n",
    "                    \n",
    "processors = {\n",
    "      \"myproc\": MyProcess\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfpy3",
   "language": "python",
   "name": "tfpy3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
