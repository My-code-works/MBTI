{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Input\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "from keras_contrib.layers.crf import CRF\n",
    "from keras_contrib.utils import save_load_utils\n",
    "from keras_contrib.metrics import crf_accuracy\n",
    "from keras_contrib.losses import crf_loss\n",
    "from keras.utils import to_categorical\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "BERT_BASE = os.path.join(os.getcwd(), 'bert/bert_model/uncased_L-12_H-768_A-12')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMmodel:\n",
    "    def __init__(self, input_length, para_emb_dim, num_tags, hidden_dim=200, dropout=0.5):\n",
    "        self.num_tags = num_tags\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Bidirectional(LSTM(hidden_dim, return_sequences=True), input_shape=(input_length, para_emb_dim)))\n",
    "        self.model.add(Dropout(dropout))\n",
    "        # self.model.add(Bidirectional(LSTM(hidden_dim, return_sequences=True), input_shape=(input_length, para_emb_dim)))\n",
    "        # self.model.add(Dropout(dropout))\n",
    "        self.model.add(TimeDistributed(Dense(self.num_tags)))\n",
    "        crf = CRF(self.num_tags)\n",
    "        self.model.add(crf)\n",
    "        self.model.compile('rmsprop', loss=crf_loss, metrics=[crf_accuracy])\n",
    "    \n",
    "    def save_model(self, filepath):\n",
    "        save_load_utils.save_all_weights(self.model, filepath)\n",
    "    \n",
    "    def restore_model(self, filepath):\n",
    "        save_load_utils.load_all_weights(self.model, filepath)\n",
    "        \n",
    "    def train(self, trainX, trainY, batch_size=32, epochs=10, validation_split=0.1, verbose=1):\n",
    "        return self.model.fit(trainX, np.array(trainY), batch_size=batch_size, epochs=epochs, \n",
    "                             validation_split=validation_split, verbose=verbose)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading 0.json...\n",
      "loading 1.json...\n",
      "loading 2.json...\n",
      "loading 3.json...\n",
      "loading 4.json...\n",
      "loading 5.json...\n",
      "loading 6.json...\n",
      "loading 7.json...\n",
      "loading 8.json...\n",
      "loading 9.json...\n",
      "loading 10.json...\n",
      "loading 11.json...\n",
      "loading 12.json...\n",
      "loading 13.json...\n",
      "loading 14.json...\n",
      "loading 15.json...\n",
      "loading 16.json...\n",
      "loading 17.json...\n",
      "loading 18.json...\n",
      "loading 19.json...\n",
      "loading 20.json...\n",
      "loading 21.json...\n",
      "loading 22.json...\n",
      "loading 23.json...\n",
      "loading 24.json...\n",
      "loading 25.json...\n",
      "loading 26.json...\n",
      "loading 27.json...\n",
      "loading 28.json...\n",
      "loading 29.json...\n",
      "loading 30.json...\n",
      "loading 31.json...\n",
      "loading 32.json...\n",
      "loading 33.json...\n",
      "loading 34.json...\n",
      "loading 35.json...\n",
      "loading 36.json...\n",
      "loading 37.json...\n",
      "loading 38.json...\n",
      "loading 39.json...\n",
      "loading 40.json...\n",
      "loading 41.json...\n",
      "loading 42.json...\n",
      "loading 43.json...\n",
      "loading 44.json...\n",
      "loading 45.json...\n",
      "loading 46.json...\n",
      "loading 47.json...\n",
      "loading 48.json...\n",
      "loading 49.json...\n",
      "loading 50.json...\n",
      "loading 51.json...\n",
      "loading 52.json...\n",
      "loading 53.json...\n",
      "loading 54.json...\n",
      "loading 55.json...\n",
      "loading 56.json...\n",
      "loading 57.json...\n",
      "loading 58.json...\n",
      "loading 59.json...\n",
      "loading 60.json...\n",
      "loading 61.json...\n",
      "loading 62.json...\n",
      "loading 63.json...\n",
      "loading 64.json...\n",
      "loading 65.json...\n",
      "loading 66.json...\n",
      "loading 67.json...\n",
      "loading 68.json...\n",
      "loading 69.json...\n",
      "loading 70.json...\n",
      "loading 71.json...\n",
      "loading 72.json...\n",
      "loading 73.json...\n",
      "loading 74.json...\n",
      "loading 75.json...\n",
      "loading 76.json...\n",
      "loading 77.json...\n",
      "loading 78.json...\n",
      "loading 79.json...\n",
      "loading 80.json...\n",
      "loading 81.json...\n",
      "loading 82.json...\n",
      "loading 83.json...\n",
      "loading 84.json...\n",
      "loading 85.json...\n",
      "loading 86.json...\n",
      "loading 87.json...\n",
      "loading 88.json...\n",
      "loading 89.json...\n",
      "loading 90.json...\n",
      "loading 91.json...\n",
      "loading 92.json...\n",
      "loading 93.json...\n",
      "loading 94.json...\n",
      "loading 95.json...\n",
      "loading 96.json...\n",
      "loading 97.json...\n",
      "loading 98.json...\n",
      "loading 99.json...\n",
      "loading 100.json...\n",
      "loading 101.json...\n",
      "loading 102.json...\n",
      "loading 103.json...\n",
      "loading 104.json...\n",
      "loading 105.json...\n",
      "loading 106.json...\n",
      "loading 107.json...\n",
      "loading 108.json...\n",
      "loading 109.json...\n",
      "loading 110.json...\n",
      "loading 111.json...\n",
      "loading 112.json...\n",
      "loading 113.json...\n",
      "loading 114.json...\n",
      "loading 115.json...\n",
      "loading 116.json...\n",
      "loading 117.json...\n",
      "loading 118.json...\n",
      "loading 119.json...\n",
      "loading 120.json...\n",
      "loading 121.json...\n",
      "loading 122.json...\n",
      "loading 123.json...\n",
      "loading 124.json...\n",
      "loading 125.json...\n",
      "loading 126.json...\n",
      "loading 127.json...\n",
      "loading 128.json...\n",
      "loading 129.json...\n",
      "loading 130.json...\n",
      "loading 131.json...\n",
      "loading 132.json...\n",
      "loading 133.json...\n",
      "loading 134.json...\n",
      "loading 135.json...\n",
      "loading 136.json...\n",
      "loading 137.json...\n",
      "loading 138.json...\n",
      "loading 139.json...\n",
      "loading 140.json...\n",
      "loading 141.json...\n",
      "loading 142.json...\n",
      "loading 143.json...\n",
      "loading 144.json...\n",
      "loading 145.json...\n",
      "loading 146.json...\n",
      "loading 147.json...\n",
      "loading 148.json...\n",
      "loading 149.json...\n",
      "loading 150.json...\n",
      "loading 151.json...\n",
      "loading 152.json...\n",
      "loading 153.json...\n",
      "loading 154.json...\n",
      "loading 155.json...\n",
      "loading 156.json...\n",
      "loading 157.json...\n",
      "loading 158.json...\n",
      "loading 159.json...\n",
      "loading 160.json...\n",
      "loading 161.json...\n",
      "loading 162.json...\n",
      "loading 163.json...\n",
      "loading 164.json...\n",
      "loading 165.json...\n",
      "loading 166.json...\n",
      "loading 167.json...\n",
      "loading 168.json...\n",
      "loading 169.json...\n",
      "loading 170.json...\n",
      "loading 171.json...\n",
      "loading 172.json...\n",
      "loading 173.json...\n",
      "loading 174.json...\n",
      "loading 175.json...\n",
      "loading 176.json...\n",
      "loading 177.json...\n",
      "loading 178.json...\n",
      "loading 179.json...\n",
      "loading 180.json...\n",
      "loading 181.json...\n",
      "loading 182.json...\n",
      "loading 183.json...\n",
      "loading 184.json...\n",
      "loading 185.json...\n",
      "loading 186.json...\n",
      "loading 187.json...\n",
      "loading 188.json...\n",
      "loading 189.json...\n",
      "loading 190.json...\n",
      "loading 191.json...\n",
      "loading 192.json...\n",
      "loading 193.json...\n",
      "loading 194.json...\n",
      "loading 195.json...\n",
      "loading 196.json...\n",
      "loading 197.json...\n",
      "loading 198.json...\n",
      "loading 199.json...\n",
      "loading 200.json...\n",
      "loading 201.json...\n",
      "loading 202.json...\n",
      "loading 203.json...\n",
      "loading 204.json...\n",
      "loading 205.json...\n",
      "loading 206.json...\n",
      "loading 207.json...\n",
      "loading 208.json...\n",
      "loading 209.json...\n",
      "loading 210.json...\n",
      "loading 211.json...\n",
      "loading 212.json...\n",
      "loading 213.json...\n",
      "loading 214.json...\n",
      "loading 215.json...\n",
      "loading 216.json...\n",
      "loading 217.json...\n",
      "loading 218.json...\n",
      "loading 219.json...\n",
      "loading 220.json...\n",
      "loading 221.json...\n",
      "loading 222.json...\n",
      "loading 223.json...\n",
      "loading 224.json...\n",
      "loading 225.json...\n",
      "loading 226.json...\n",
      "loading 227.json...\n",
      "loading 228.json...\n",
      "loading 229.json...\n",
      "loading 230.json...\n",
      "loading 231.json...\n",
      "loading 232.json...\n",
      "loading 233.json...\n",
      "loading 234.json...\n",
      "loading 235.json...\n",
      "loading 236.json...\n",
      "loading 237.json...\n",
      "loading 238.json...\n",
      "loading 239.json...\n",
      "loading 240.json...\n",
      "loading 241.json...\n",
      "loading 242.json...\n",
      "loading 243.json...\n",
      "loading 244.json...\n",
      "loading 245.json...\n",
      "loading 246.json...\n",
      "loading 247.json...\n",
      "loading 248.json...\n",
      "loading 249.json...\n",
      "loading 250.json...\n",
      "loading 251.json...\n",
      "loading 252.json...\n",
      "loading 253.json...\n",
      "loading 254.json...\n",
      "loading 255.json...\n",
      "loading 256.json...\n",
      "loading 257.json...\n",
      "loading 258.json...\n",
      "loading 259.json...\n",
      "loading 260.json...\n",
      "loading 261.json...\n",
      "loading 262.json...\n",
      "loading 263.json...\n",
      "loading 264.json...\n",
      "loading 265.json...\n",
      "loading 266.json...\n",
      "loading 267.json...\n",
      "loading 268.json...\n",
      "loading 269.json...\n",
      "loading 270.json...\n",
      "loading 271.json...\n",
      "loading 272.json...\n",
      "loading 273.json...\n",
      "loading 274.json...\n",
      "loading 275.json...\n",
      "loading 276.json...\n",
      "loading 277.json...\n",
      "loading 278.json...\n",
      "loading 279.json...\n",
      "loading 280.json...\n",
      "loading 281.json...\n",
      "loading 282.json...\n",
      "loading 283.json...\n",
      "loading 284.json...\n",
      "loading 285.json...\n",
      "loading 286.json...\n",
      "loading 287.json...\n",
      "loading 288.json...\n",
      "loading 289.json...\n",
      "loading 290.json...\n",
      "loading 291.json...\n",
      "loading 292.json...\n",
      "loading 293.json...\n",
      "loading 294.json...\n",
      "loading 295.json...\n",
      "loading 296.json...\n",
      "loading 297.json...\n",
      "loading 298.json...\n",
      "loading 299.json...\n",
      "loading 300.json...\n",
      "loading 301.json...\n",
      "loading 302.json...\n",
      "loading 303.json...\n",
      "loading 304.json...\n",
      "loading 305.json...\n",
      "loading 306.json...\n",
      "loading 307.json...\n",
      "loading 308.json...\n",
      "loading 309.json...\n",
      "loading 310.json...\n",
      "loading 311.json...\n",
      "loading 312.json...\n",
      "loading 313.json...\n",
      "loading 314.json...\n",
      "loading 315.json...\n",
      "loading 316.json...\n",
      "loading 317.json...\n",
      "loading 318.json...\n",
      "loading 319.json...\n",
      "loading 320.json...\n",
      "loading 321.json...\n",
      "loading 322.json...\n",
      "loading 323.json...\n",
      "loading 324.json...\n",
      "loading 325.json...\n",
      "loading 326.json...\n",
      "loading 327.json...\n",
      "loading 328.json...\n",
      "loading 329.json...\n",
      "loading 330.json...\n",
      "loading 331.json...\n",
      "loading 332.json...\n",
      "loading 333.json...\n",
      "loading 334.json...\n",
      "loading 335.json...\n",
      "loading 336.json...\n",
      "loading 337.json...\n",
      "loading 338.json...\n",
      "loading 339.json...\n",
      "loading 340.json...\n",
      "loading 341.json...\n",
      "loading 342.json...\n",
      "loading 343.json...\n",
      "loading 344.json...\n",
      "loading 345.json...\n",
      "loading 346.json...\n",
      "loading 347.json...\n",
      "loading 348.json...\n",
      "loading 349.json...\n",
      "loading 350.json...\n",
      "loading 351.json...\n",
      "loading 352.json...\n",
      "loading 353.json...\n",
      "loading 354.json...\n",
      "loading 355.json...\n",
      "loading 356.json...\n",
      "loading 357.json...\n",
      "loading 358.json...\n",
      "loading 359.json...\n",
      "loading 360.json...\n",
      "loading 361.json...\n",
      "loading 362.json...\n",
      "loading 363.json...\n",
      "loading 364.json...\n",
      "loading 365.json...\n",
      "loading 366.json...\n",
      "loading 367.json...\n",
      "loading 368.json...\n",
      "loading 369.json...\n",
      "loading 370.json...\n",
      "loading 371.json...\n",
      "loading 372.json...\n",
      "loading 373.json...\n",
      "loading 374.json...\n",
      "loading 375.json...\n",
      "loading 376.json...\n",
      "loading 377.json...\n",
      "loading 378.json...\n",
      "loading 379.json...\n",
      "loading 380.json...\n",
      "loading 381.json...\n",
      "loading 382.json...\n",
      "loading 383.json...\n",
      "loading 384.json...\n",
      "loading 385.json...\n",
      "loading 386.json...\n",
      "loading 387.json...\n",
      "loading 388.json...\n",
      "loading 389.json...\n",
      "loading 390.json...\n",
      "loading 391.json...\n",
      "loading 392.json...\n",
      "loading 393.json...\n",
      "loading 394.json...\n",
      "loading 395.json...\n",
      "loading 396.json...\n",
      "loading 397.json...\n",
      "loading 398.json...\n",
      "loading 399.json...\n",
      "loading 400.json...\n",
      "loading 401.json...\n",
      "loading 402.json...\n",
      "loading 403.json...\n",
      "loading 404.json...\n",
      "loading 405.json...\n",
      "loading 406.json...\n",
      "loading 407.json...\n",
      "loading 408.json...\n",
      "loading 409.json...\n",
      "loading 410.json...\n",
      "loading 411.json...\n",
      "loading 412.json...\n",
      "loading 413.json...\n",
      "loading 414.json...\n",
      "loading 415.json...\n",
      "loading 416.json...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading 417.json...\n",
      "loading 418.json...\n",
      "loading 419.json...\n",
      "loading 420.json...\n",
      "loading 421.json...\n",
      "loading 422.json...\n",
      "loading 423.json...\n",
      "loading 424.json...\n",
      "loading 425.json...\n",
      "loading 426.json...\n",
      "loading 427.json...\n",
      "loading 428.json...\n",
      "loading 429.json...\n",
      "loading 430.json...\n",
      "loading 431.json...\n",
      "loading 432.json...\n",
      "loading 433.json...\n",
      "loading 434.json...\n",
      "loading 435.json...\n",
      "loading 436.json...\n",
      "loading 437.json...\n",
      "loading 438.json...\n",
      "loading 439.json...\n",
      "loading 440.json...\n",
      "loading 441.json...\n",
      "loading 442.json...\n",
      "loading 443.json...\n",
      "loading 444.json...\n",
      "loading 445.json...\n",
      "loading 446.json...\n",
      "loading 447.json...\n",
      "loading 448.json...\n",
      "loading 449.json...\n",
      "loading 450.json...\n",
      "loading 451.json...\n",
      "loading 452.json...\n",
      "loading 453.json...\n",
      "loading 454.json...\n",
      "loading 455.json...\n",
      "loading 456.json...\n",
      "loading 457.json...\n",
      "loading 458.json...\n",
      "loading 459.json...\n",
      "loading 460.json...\n",
      "loading 461.json...\n",
      "loading 462.json...\n",
      "loading 463.json...\n",
      "loading 464.json...\n",
      "loading 465.json...\n",
      "loading 466.json...\n",
      "loading 467.json...\n",
      "loading 468.json...\n",
      "loading 469.json...\n",
      "loading 470.json...\n",
      "loading 471.json...\n",
      "loading 472.json...\n",
      "loading 473.json...\n",
      "loading 474.json...\n",
      "loading 475.json...\n",
      "loading 476.json...\n",
      "loading 477.json...\n",
      "loading 478.json...\n",
      "loading 479.json...\n",
      "loading 480.json...\n",
      "loading 481.json...\n",
      "loading 482.json...\n",
      "loading 483.json...\n",
      "loading 484.json...\n",
      "loading 485.json...\n",
      "loading 486.json...\n",
      "loading 487.json...\n",
      "loading 488.json...\n",
      "loading 489.json...\n",
      "loading 490.json...\n",
      "loading 491.json...\n",
      "loading 492.json...\n",
      "loading 493.json...\n",
      "loading 494.json...\n",
      "loading 495.json...\n",
      "loading 496.json...\n",
      "loading 497.json...\n",
      "loading 498.json...\n",
      "loading 499.json...\n",
      "Finish loading data! Total 114975 articles.\n"
     ]
    }
   ],
   "source": [
    "from Dataprocessor import Dataprocessor\n",
    "\n",
    "filelist = [('data/%d.json' % i) for i in range(500)]\n",
    "processor = Dataprocessor()\n",
    "train_texts, train_tags, train_rawtags = processor.load_data(filelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 82972 paragraphs\n",
      "WARNING:tensorflow:Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7fd8f858c1e0>) includes params argument, but params are not passed to Estimator.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpfz9nntj2\n",
      "WARNING:tensorflow:eval_on_tpu ignored because use_tpu is False.\n",
      "0 ...\n",
      "2000 ...\n",
      "4000 ...\n",
      "6000 ...\n",
      "8000 ...\n",
      "10000 ...\n",
      "12000 ...\n",
      "14000 ...\n",
      "16000 ...\n",
      "18000 ...\n",
      "20000 ...\n",
      "22000 ...\n",
      "24000 ...\n",
      "26000 ...\n",
      "28000 ...\n",
      "30000 ...\n",
      "32000 ...\n",
      "34000 ...\n",
      "36000 ...\n",
      "38000 ...\n",
      "40000 ...\n",
      "42000 ...\n",
      "44000 ...\n",
      "46000 ...\n",
      "48000 ...\n",
      "50000 ...\n",
      "52000 ...\n",
      "54000 ...\n",
      "56000 ...\n",
      "58000 ...\n",
      "60000 ...\n",
      "62000 ...\n",
      "64000 ...\n",
      "66000 ...\n",
      "68000 ...\n",
      "70000 ...\n",
      "72000 ...\n",
      "74000 ...\n",
      "76000 ...\n",
      "78000 ...\n",
      "80000 ...\n",
      "82000 ...\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "from bert_utils import get_all_features\n",
    "\n",
    "bert_config_file = os.path.join(BERT_BASE, 'bert_config.json')\n",
    "vocab_file = os.path.join(BERT_BASE, 'vocab.txt')\n",
    "bert_checkpoint = os.path.join(BERT_BASE, 'bert_model.ckpt')\n",
    "    \n",
    "feature = get_all_features(train_texts[0:2000], bert_config_file, vocab_file, bert_checkpoint)\n",
    "print(len(feature))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 166422 paragraphs\n",
      "WARNING:tensorflow:Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7fd8547e4e18>) includes params argument, but params are not passed to Estimator.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp115o0n0v\n",
      "WARNING:tensorflow:eval_on_tpu ignored because use_tpu is False.\n",
      "0 ...\n",
      "2000 ...\n",
      "4000 ...\n",
      "6000 ...\n",
      "8000 ...\n",
      "10000 ...\n",
      "12000 ...\n",
      "14000 ...\n",
      "16000 ...\n",
      "18000 ...\n",
      "20000 ...\n",
      "22000 ...\n",
      "24000 ...\n",
      "26000 ...\n",
      "28000 ...\n",
      "30000 ...\n",
      "32000 ...\n",
      "34000 ...\n",
      "36000 ...\n",
      "38000 ...\n",
      "40000 ...\n",
      "42000 ...\n",
      "44000 ...\n",
      "46000 ...\n",
      "48000 ...\n",
      "50000 ...\n",
      "52000 ...\n",
      "54000 ...\n",
      "56000 ...\n",
      "58000 ...\n",
      "60000 ...\n",
      "62000 ...\n",
      "64000 ...\n",
      "66000 ...\n",
      "68000 ...\n",
      "70000 ...\n",
      "72000 ...\n",
      "74000 ...\n",
      "76000 ...\n",
      "78000 ...\n",
      "80000 ...\n",
      "82000 ...\n",
      "84000 ...\n",
      "86000 ...\n",
      "88000 ...\n",
      "90000 ...\n",
      "92000 ...\n",
      "94000 ...\n",
      "96000 ...\n",
      "98000 ...\n",
      "100000 ...\n",
      "102000 ...\n",
      "104000 ...\n",
      "106000 ...\n",
      "108000 ...\n",
      "110000 ...\n",
      "112000 ...\n",
      "114000 ...\n",
      "116000 ...\n",
      "118000 ...\n",
      "120000 ...\n",
      "122000 ...\n",
      "124000 ...\n",
      "126000 ...\n",
      "128000 ...\n",
      "130000 ...\n",
      "132000 ...\n",
      "134000 ...\n",
      "136000 ...\n",
      "138000 ...\n",
      "140000 ...\n",
      "142000 ...\n",
      "144000 ...\n",
      "146000 ...\n",
      "148000 ...\n",
      "150000 ...\n",
      "152000 ...\n",
      "154000 ...\n",
      "156000 ...\n",
      "158000 ...\n",
      "160000 ...\n",
      "162000 ...\n",
      "164000 ...\n",
      "166000 ...\n",
      "6000\n"
     ]
    }
   ],
   "source": [
    "feature += get_all_features(train_texts[2000:6000], bert_config_file, vocab_file, bert_checkpoint)\n",
    "print(len(feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_3 (Bidirection (None, 100, 400)          1550400   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100, 400)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 100, 12)           4812      \n",
      "_________________________________________________________________\n",
      "crf_3 (CRF)                  (None, 100, 12)           324       \n",
      "=================================================================\n",
      "Total params: 1,555,536\n",
      "Trainable params: 1,555,536\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "INPUT_LENGTH = 100\n",
    "PARAGRAPH_EMB_DIM = 768\n",
    "NUM_TAGS = 12\n",
    "\n",
    "model = LSTMmodel(INPUT_LENGTH, PARAGRAPH_EMB_DIM, NUM_TAGS)\n",
    "model.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "import numpy as np\n",
    "\n",
    "tags = train_tags[0:6000]\n",
    "X, rawY = [], [] # X is 3D: article, paragraph, embedding; Y is 2D: article, paragraph\n",
    "for f, t in zip(feature, tags):\n",
    "    while len(f) < INPUT_LENGTH:\n",
    "        f.append(np.zeros(PARAGRAPH_EMB_DIM))\n",
    "        t.append(0)\n",
    "    f = f[0:INPUT_LENGTH]\n",
    "    t = t[0:INPUT_LENGTH]\n",
    "    X.append(f)\n",
    "    rawY.append(t)\n",
    "    \n",
    "Y = [to_categorical(y, num_classes=NUM_TAGS) for y in rawY] # Y is now 3D\n",
    "\n",
    "data_size = len(X)\n",
    "train_size = int(data_size * 0.9)\n",
    "trainX, trainY = X[:train_size], Y[:train_size]\n",
    "testX, testY = X[train_size:], Y[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1620 samples, validate on 180 samples\n",
      "Epoch 1/10\n",
      "1620/1620 [==============================] - 21s 13ms/step - loss: 0.2261 - crf_accuracy: 0.9032 - val_loss: 0.2092 - val_crf_accuracy: 0.9059\n",
      "Epoch 2/10\n",
      "1620/1620 [==============================] - 21s 13ms/step - loss: 0.2163 - crf_accuracy: 0.9047 - val_loss: 0.2179 - val_crf_accuracy: 0.9002\n",
      "Epoch 3/10\n",
      "1620/1620 [==============================] - 22s 13ms/step - loss: 0.2099 - crf_accuracy: 0.9063 - val_loss: 0.2051 - val_crf_accuracy: 0.9078\n",
      "Epoch 4/10\n",
      "1620/1620 [==============================] - 21s 13ms/step - loss: 0.2036 - crf_accuracy: 0.9079 - val_loss: 0.2045 - val_crf_accuracy: 0.9049\n",
      "Epoch 5/10\n",
      "1620/1620 [==============================] - 21s 13ms/step - loss: 0.1966 - crf_accuracy: 0.9093 - val_loss: 0.2069 - val_crf_accuracy: 0.8997\n",
      "Epoch 6/10\n",
      "1620/1620 [==============================] - 21s 13ms/step - loss: 0.1904 - crf_accuracy: 0.9109 - val_loss: 0.1958 - val_crf_accuracy: 0.9026\n",
      "Epoch 7/10\n",
      "1620/1620 [==============================] - 21s 13ms/step - loss: 0.1850 - crf_accuracy: 0.9126 - val_loss: 0.1969 - val_crf_accuracy: 0.9032\n",
      "Epoch 8/10\n",
      "1620/1620 [==============================] - 21s 13ms/step - loss: 0.1779 - crf_accuracy: 0.9145 - val_loss: 0.1923 - val_crf_accuracy: 0.9073\n",
      "Epoch 9/10\n",
      "1620/1620 [==============================] - 21s 13ms/step - loss: 0.1738 - crf_accuracy: 0.9151 - val_loss: 0.1900 - val_crf_accuracy: 0.9072\n",
      "Epoch 10/10\n",
      "1620/1620 [==============================] - 21s 13ms/step - loss: 0.1666 - crf_accuracy: 0.9174 - val_loss: 0.1875 - val_crf_accuracy: 0.9057\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "history = model.model.fit(np.array(trainX), np.array(trainY), batch_size=32, epochs=10, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "# plt.style.use(\"ggplot\")\n",
    "# plt.figure(figsize=(12,12))\n",
    "# plt.plot(hist[\"acc\"])\n",
    "# plt.plot(hist[\"val_acc\"])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 2s 8ms/step\n"
     ]
    }
   ],
   "source": [
    "# Predict on test\n",
    "test_pred = model.model.predict(np.array(testX), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4621 2201 0.6773673409557315\n"
     ]
    }
   ],
   "source": [
    "truecnt = 0\n",
    "falsecnt = 0\n",
    "for (i, pred) in enumerate(test_pred):\n",
    "    for j, p in enumerate(pred):\n",
    "        if np.argmax(testY[i][j]) != 0:\n",
    "            if np.argmax(p) == np.argmax(testY[i][j]):\n",
    "                truecnt += 1\n",
    "            else:\n",
    "                falsecnt += 1\n",
    "print(truecnt, falsecnt, truecnt/(truecnt+falsecnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfpy3",
   "language": "python",
   "name": "tfpy3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
